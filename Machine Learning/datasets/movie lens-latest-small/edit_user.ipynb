{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1                   Adventure|Children|Fantasy  \n",
      "2                               Comedy|Romance  \n",
      "3                         Comedy|Drama|Romance  \n",
      "4                                       Comedy  \n",
      "   userId  movieId  rating  timestamp\n",
      "0       1        1     4.0  964982703\n",
      "1       1        3     4.0  964981247\n",
      "2       1        6     4.0  964982224\n",
      "3       1       47     5.0  964983815\n",
      "4       1       50     5.0  964982931\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('movies copy.csv')\n",
    "df2 = pd.read_csv('ratings copy.csv')\n",
    "print(df1.head())\n",
    "print(df2.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n"
     ]
    }
   ],
   "source": [
    "listMovieId = df1[\"movieId\"].unique()\n",
    "k = (df2[:2000])\n",
    "newDf = (k.query(\"movieId in @listMovieId\"))\n",
    "print(len(newDf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save New Datasets 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDf.head()\n",
    "newDf.to_csv(\"ratings 200.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"movies 200.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFDS CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: tfds build [-h] [--helpfull]\n",
      "                  [--datasets DATASETS_KEYWORD [DATASETS_KEYWORD ...]]\n",
      "                  [--overwrite]\n",
      "                  [--max_examples_per_split [MAX_EXAMPLES_PER_SPLIT]]\n",
      "                  [--data_dir DATA_DIR] [--download_dir DOWNLOAD_DIR]\n",
      "                  [--extract_dir EXTRACT_DIR] [--manual_dir MANUAL_DIR]\n",
      "                  [--add_name_to_manual_dir] [--config CONFIG]\n",
      "                  [--config_idx CONFIG_IDX] [--imports IMPORTS]\n",
      "                  [--register_checksums] [--force_checksums_validation]\n",
      "                  [--beam_pipeline_options BEAM_PIPELINE_OPTIONS]\n",
      "                  [--file_format FILE_FORMAT]\n",
      "                  [--exclude_datasets EXCLUDE_DATASETS]\n",
      "                  [--experimental_latest_version]\n",
      "                  [datasets ...]\n",
      "\n",
      "positional arguments:\n",
      "  datasets              Name(s) of the dataset(s) to build. Default to current\n",
      "                        dir. See https://www.tensorflow.org/datasets/cli for\n",
      "                        accepted values.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --helpfull            show full help message and exit\n",
      "  --datasets DATASETS_KEYWORD [DATASETS_KEYWORD ...]\n",
      "                        Datasets can also be provided as keyword argument.\n",
      "\n",
      "Debug & tests:\n",
      "  --pdb Enter post-mortem debugging mode if an exception is raised.\n",
      "\n",
      "  --overwrite           Delete pre-existing dataset if it exists.\n",
      "  --max_examples_per_split [MAX_EXAMPLES_PER_SPLIT]\n",
      "                        When set, only generate the first X examples (default\n",
      "                        to 1), rather than the full dataset.If set to 0, only\n",
      "                        execute the `_split_generators` (which download the\n",
      "                        original data), but skip `_generator_examples`\n",
      "\n",
      "Paths:\n",
      "  --data_dir DATA_DIR   Where to place datasets. Default to\n",
      "                        `~/tensorflow_datasets/` or `TFDS_DATA_DIR`\n",
      "                        environement variable.\n",
      "  --download_dir DOWNLOAD_DIR\n",
      "                        Where to place downloads. Default to\n",
      "                        `<data_dir>/downloads/`.\n",
      "  --extract_dir EXTRACT_DIR\n",
      "                        Where to extract files. Default to\n",
      "                        `<download_dir>/extracted/`.\n",
      "  --manual_dir MANUAL_DIR\n",
      "                        Where to manually download data (required for some\n",
      "                        datasets). Default to `<download_dir>/manual/`.\n",
      "  --add_name_to_manual_dir\n",
      "                        If true, append the dataset name to the `manual_dir`\n",
      "                        (e.g. `<download_dir>/manual/<dataset_name>/`. Useful\n",
      "                        to avoid collisions if many datasets are generated.\n",
      "\n",
      "Generation:\n",
      "  --config CONFIG, -c CONFIG\n",
      "                        Config name to build. Build all configs if not set.\n",
      "  --config_idx CONFIG_IDX\n",
      "                        Config id to build\n",
      "                        (`builder_cls.BUILDER_CONFIGS[config_idx]`). Mutually\n",
      "                        exclusive with `--config`.\n",
      "  --imports IMPORTS, -i IMPORTS\n",
      "                        Comma separated list of module to import to register\n",
      "                        datasets.\n",
      "  --register_checksums  If True, store size and checksum of downloaded files.\n",
      "  --force_checksums_validation\n",
      "                        If True, raise an error if the checksums are not\n",
      "                        found.\n",
      "  --beam_pipeline_options BEAM_PIPELINE_OPTIONS\n",
      "                        A (comma-separated) list of flags to pass to\n",
      "                        `PipelineOptions` when preparing with Apache Beam.\n",
      "                        (see:\n",
      "                        https://www.tensorflow.org/datasets/beam_datasets).\n",
      "                        Example: `--beam_pipeline_options=job_name=my-\n",
      "                        job,project=my-project`\n",
      "  --file_format FILE_FORMAT\n",
      "                        File format to which generate the tf-examples.\n",
      "                        Available values: ['tfrecord', 'riegeli'] (see\n",
      "                        `tfds.core.FileFormat`).\n",
      "\n",
      "Automation:\n",
      "  Used by automated scripts.\n",
      "\n",
      "  --exclude_datasets EXCLUDE_DATASETS\n",
      "                        If set, generate all datasets except the one defined\n",
      "                        here. Comma separated list of datasets to exclude.\n",
      "  --experimental_latest_version\n",
      "                        Build the latest Version(experiments=...) available\n",
      "                        rather than default version.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 20:28:00.580970: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-05-13 20:28:00.581488: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "!tfds build --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%env TF_CPP_MIN_LOG_LEVEL=1  # Disable logs on TF import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: tfds [-h] [--helpfull] [--version] {build,new} ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 20:31:39.889902: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tensorflow Datasets CLI tool\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help   show this help message and exit\n",
      "  --helpfull   show full help message and exit\n",
      "  --version    show program's version number and exit\n",
      "\n",
      "command:\n",
      "  {build,new}\n",
      "    build      Commands for downloading and preparing datasets.\n",
      "    new        Creates a new dataset directory from the template.\n"
     ]
    }
   ],
   "source": [
    "!tfds --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generated at C:\\Users\\Muhammad Nur Ilmi\\OneDrive\\Dokumen\\Bangkit\\Bangkit2022\\Caspstone\\Training-Platform-for-Micro-Small-and-Medium-Enterprises\\Machine Learning\\datasets\\movie lens-latest-small\\my_dataset\n",
      "You can start searching `TODO(my_dataset)` to complete the implementation.\n",
      "Please check https://www.tensorflow.org/datasets/add_dataset for additional details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 20:31:53.505981: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-05-13 20:31:57.343853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2022-05-13 20:31:57.343999: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "!tfds new my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Muhammad Nur Ilmi\\OneDrive\\Dokumen\\Bangkit\\Bangkit2022\\Caspstone\\Training-Platform-for-Micro-Small-and-Medium-Enterprises\\Machine Learning\\datasets\\movie lens-latest-small\\my_dataset\n",
      " Volume in drive C is Windows\n",
      " Volume Serial Number is 127D-5FA9\n",
      "\n",
      " Directory of c:\\Users\\Muhammad Nur Ilmi\\OneDrive\\Dokumen\\Bangkit\\Bangkit2022\\Caspstone\\Training-Platform-for-Micro-Small-and-Medium-Enterprises\\Machine Learning\\datasets\\movie lens-latest-small\\my_dataset\n",
      "\n",
      "13/05/2022  20:31    <DIR>          .\n",
      "13/05/2022  20:31    <DIR>          ..\n",
      "13/05/2022  20:31                64 __init__.py\n",
      "13/05/2022  20:31               160 checksums.tsv\n",
      "13/05/2022  20:31    <DIR>          dummy_data\n",
      "13/05/2022  20:31             2.140 my_dataset.py\n",
      "13/05/2022  20:31               745 my_dataset_test.py\n",
      "               4 File(s)          3.109 bytes\n",
      "               3 Dir(s)  175.032.270.848 bytes free\n"
     ]
    }
   ],
   "source": [
    "%cd my_dataset\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO[build.py]: Loading dataset  from path: c:\\Users\\Muhammad Nur Ilmi\\OneDrive\\Dokumen\\Bangkit\\Bangkit2022\\Caspstone\\Training-Platform-for-Micro-Small-and-Medium-Enterprises\\Machine Learning\\datasets\\movie lens-latest-small\\my_dataset\\my_dataset.py\n",
      "INFO[build.py]: download_and_prepare for dataset my_dataset/1.0.0...\n",
      "INFO[dataset_builder.py]: Generating dataset my_dataset (C:\\Users\\Muhammad Nur Ilmi\\tensorflow_datasets\\my_dataset\\1.0.0)\n",
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Muhammad Nur Ilmi\\tensorflow_datasets\\my_dataset\\1.0.0...\u001b[0m\n",
      "INFO[download_manager.py]: Downloading https://todo-data-url into C:\\Users\\Muhammad Nur Ilmi\\tensorflow_datasets\\downloads\\todo-data-urlpBUd8qQ23lt5lhA0m7PEVFkuiX1TnaXfmqZ3TdfbiFU.tmp.dcd65b5b1abb4302bba0f0cf700845cb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 20:35:45.535658: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-05-13 20:35:49.415993: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2022-05-13 20:35:49.416150: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\n",
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
      "                                       \n",
      "\n",
      "\n",
      "                                  \n",
      "\u001b[A\n",
      "\n",
      "\n",
      "                                                 \n",
      "\u001b[A\u001b[A\n",
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
      "Extraction completed...: 0 file [00:02, ? file/s]\n",
      "\n",
      "Dl Size...: 0 MiB [00:02, ? MiB/s]\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1040, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\", line 358, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x000001D76D387820>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 440, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 785, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='todo-data-url', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001D76D387820>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\Scripts\\tfds.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\scripts\\cli\\main.py\", line 102, in launch_cli\n",
      "    app.run(main, flags_parser=_parse_flags)\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 312, in run\n",
      "    _run_main(main, args)\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 258, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\scripts\\cli\\main.py\", line 97, in main\n",
      "    args.subparser_fn(args)\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\scripts\\cli\\build.py\", line 192, in _build_datasets\n",
      "    _download_and_prepare(args, builder)\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\scripts\\cli\\build.py\", line 343, in _download_and_prepare\n",
      "    builder.download_and_prepare(\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 462, in download_and_prepare\n",
      "    self._download_and_prepare(\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 1157, in _download_and_prepare\n",
      "    split_generators = self._split_generators(  # pylint: disable=unexpected-keyword-arg\n",
      "  File \"c:\\Users\\Muhammad Nur Ilmi\\OneDrive\\Dokumen\\Bangkit\\Bangkit2022\\Caspstone\\Training-Platform-for-Micro-Small-and-Medium-Enterprises\\Machine Learning\\datasets\\movie lens-latest-small\\my_dataset\\my_dataset.py\", line 48, in _split_generators\n",
      "    path = dl_manager.download_and_extract('https://todo-data-url')\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py\", line 634, in download_and_extract\n",
      "    return _map_promise(self._download_extract, url_or_urls)\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py\", line 767, in _map_promise\n",
      "    res = tf.nest.map_structure(lambda p: p.get(), all_promises)  # Wait promises\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 914, in map_structure\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 914, in <listcomp>\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py\", line 767, in <lambda>\n",
      "    res = tf.nest.map_structure(lambda p: p.get(), all_promises)  # Wait promises\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\promise\\promise.py\", line 512, in get\n",
      "    return self._target_settled_value(_raise=True)\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\promise\\promise.py\", line 516, in _target_settled_value\n",
      "    return self._target()._settled_value(_raise)\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\promise\\promise.py\", line 226, in _settled_value\n",
      "    reraise(type(raise_val), raise_val, self._traceback)\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\six.py\", line 719, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\promise\\promise.py\", line 844, in handle_future_result\n",
      "    resolve(future.result())\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\concurrent\\futures\\thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\download\\downloader.py\", line 216, in _sync_download\n",
      "    with _open_url(url, verify=verify) as (response, iter_content):\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\contextlib.py\", line 119, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\download\\downloader.py\", line 277, in _open_with_requests\n",
      "    with session.get(url, stream=True, **kwargs) as response:\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 542, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 529, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 645, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\Muhammad Nur Ilmi\\anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='todo-data-url', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001D76D387820>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n"
     ]
    }
   ],
   "source": [
    "!tfds build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9cf835102841f8dfb60b47907fa71a55b8493c47514dd57e25b2f57cd271ef6a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
